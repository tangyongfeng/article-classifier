{
  "ingest_source": {
    "id": "8df49583-3d48-4273-aee4-3aca9940ea94",
    "source_type": "evernote_html",
    "source_path": "backups/2023年6月/IT技术/强化学习.html",
    "collected_at": "2025-11-10T15:43:32.623084Z",
    "external_id": null,
    "title_hint": null,
    "language_hint": "zh-cn",
    "captured_at": null,
    "checksum": "f2560e66b43d9eafa38be26ed85fba3e62b76acf29ec845a8ecac757e3dd7efd",
    "status": "pending",
    "notes": {
      "batch_id": "phase2-backfill-202306"
    }
  },
  "note": {
    "id": "52a0ea62-550a-4b7a-b9cc-02fe7590d5a1",
    "ingest_source_id": "8df49583-3d48-4273-aee4-3aca9940ea94",
    "canonical_title": "强化学习",
    "language": "zh-cn",
    "ingested_at": "2025-11-10T15:43:32.623088Z",
    "created_at": null,
    "status": "active",
    "importance": 0,
    "attributes": {
      "source_filename": "强化学习.html"
    }
  },
  "variants": [
    {
      "id": "ca986b02-533e-43d0-95d0-799c206e7a1c",
      "note_id": "52a0ea62-550a-4b7a-b9cc-02fe7590d5a1",
      "variant_type": "raw_html",
      "version": 1,
      "created_by": "evernote_ingest:v0",
      "created_at": "2025-11-10T15:43:32.623092Z",
      "content": "---\ntitle: 强化学习\nupdated: 2017-12-20 06:06:59Z\ncreated: 2017-12-20 05:55:36Z\nauthor: tangyongfeng@gmail.com\ntags:\n  - ai\n  - 机器学习\n---\n\n\n<en-note><div>强化学习是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或者惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如：博弈论、控制论、运筹学、信息论、仿真优化、多主体系统学习、群体智能、统计学以及遗传算法。在运筹学和控制理论研究的语境下，强化学习被称为&ldquo;近似动态规划&rdquo;（approximate dynamic programming,ADP)。在最优控制理论中，也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在经济学和博弈论中，强化学习被用来解释在有限理性的条件下如何出现平衡。</div><div>在机器学习问题中，环境通常被规范为马可夫决策过程（mdp），所以许多强化学习算法在这种情况下使用动态规划技巧。传统的技术和强化学习算法的主要区别是，后者不需要关于mdp的只是，而且针对无法找到确切方法的大规模mdp。</div><div>强化学习和标准的监督式学习之间的区别在于，它并不雪瑶正确的输入、输出 对，也不需要精准矫正次优化的行为。强化学习更加专注在于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。强化学习中的&ldquo;探索-遵从&rdquo;的交换，在多臂老虎机问题和有限mdp中研究的最多。</div></en-note>      ",
      "content_path": null,
      "diff_base_variant_id": null,
      "metadata": {
        "checksum": "f2560e66b43d9eafa38be26ed85fba3e62b76acf29ec845a8ecac757e3dd7efd",
        "path": "backups/2023年6月/IT技术/强化学习.html"
      }
    },
    {
      "id": "8d7f0835-2285-4b21-bdac-7f6d3ac938c3",
      "note_id": "52a0ea62-550a-4b7a-b9cc-02fe7590d5a1",
      "variant_type": "clean_text",
      "version": 1,
      "created_by": "evernote_ingest:v0",
      "created_at": "2025-11-10T15:43:32.623096Z",
      "content": "---\ntitle: 强化学习\nupdated: 2017-12-20 06:06:59Z\ncreated: 2017-12-20 05:55:36Z\nauthor: tangyongfeng@gmail.com\ntags:\n  - ai\n  - 机器学习\n---\n\n强化学习是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或者惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如：博弈论、控制论、运筹学、信息论、仿真优化、多主体系统学习、群体智能、统计学以及遗传算法。在运筹学和控制理论研究的语境下，强化学习被称为“近似动态规划”（approximate dynamic programming,ADP)。在最优控制理论中，也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在经济学和博弈论中，强化学习被用来解释在有限理性的条件下如何出现平衡。\n在机器学习问题中，环境通常被规范为马可夫决策过程（mdp），所以许多强化学习算法在这种情况下使用动态规划技巧。传统的技术和强化学习算法的主要区别是，后者不需要关于mdp的只是，而且针对无法找到确切方法的大规模mdp。\n强化学习和标准的监督式学习之间的区别在于，它并不雪瑶正确的输入、输出 对，也不需要精准矫正次优化的行为。强化学习更加专注在于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。强化学习中的“探索-遵从”的交换，在多臂老虎机问题和有限mdp中研究的最多。",
      "content_path": null,
      "diff_base_variant_id": null,
      "metadata": {
        "language": "zh-cn",
        "length": 712,
        "rule_count": 0,
        "applied_rules": []
      }
    }
  ],
  "extractions": [],
  "journal": {
    "id": "71dd76d4-93d0-45e7-b21d-c3f7db7c793e",
    "note_id": "52a0ea62-550a-4b7a-b9cc-02fe7590d5a1",
    "stage": "ingest",
    "agent_id": "evernote_ingest:v0",
    "started_at": "2025-11-10T15:43:32.623104Z",
    "finished_at": "2025-11-10T15:43:32.623104Z",
    "status": "success",
    "input_ref": {
      "task_id": "0916b6a9-7b4e-448c-995d-8f4c628398b6",
      "source_path": "backups/2023年6月/IT技术/强化学习.html",
      "checksum": "f2560e66b43d9eafa38be26ed85fba3e62b76acf29ec845a8ecac757e3dd7efd"
    },
    "output_ref": {
      "ingest_source": "8df49583-3d48-4273-aee4-3aca9940ea94",
      "note": "52a0ea62-550a-4b7a-b9cc-02fe7590d5a1",
      "variants": [
        "ca986b02-533e-43d0-95d0-799c206e7a1c",
        "8d7f0835-2285-4b21-bdac-7f6d3ac938c3"
      ]
    },
    "error_detail": null
  }
}