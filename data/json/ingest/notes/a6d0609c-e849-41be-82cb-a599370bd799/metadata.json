{
  "ingest_source": {
    "id": "dc1d0f8c-15f2-4141-9b9a-b0e97d77a453",
    "source_type": "evernote_html",
    "source_path": "backups/2023年6月/IT技术/利用爬虫技术能做到哪些很酷很有趣很有用的事情？.html",
    "collected_at": "2025-11-10T15:43:32.250280Z",
    "external_id": null,
    "title_hint": null,
    "language_hint": "en",
    "captured_at": null,
    "checksum": "447d390c556476b2de5905e2617ac2e7d854a594bfc6e7f8253de9353322f615",
    "status": "pending",
    "notes": {
      "batch_id": "phase2-backfill-202306"
    }
  },
  "note": {
    "id": "a6d0609c-e849-41be-82cb-a599370bd799",
    "ingest_source_id": "dc1d0f8c-15f2-4141-9b9a-b0e97d77a453",
    "canonical_title": "利用爬虫技术能做到哪些很酷很有趣很有用的事情？",
    "language": "en",
    "ingested_at": "2025-11-10T15:43:32.250284Z",
    "created_at": null,
    "status": "active",
    "importance": 0,
    "attributes": {
      "source_filename": "利用爬虫技术能做到哪些很酷很有趣很有用的事情？.html"
    }
  },
  "variants": [
    {
      "id": "366e2097-307f-4847-b0a6-a52a62e2375d",
      "note_id": "a6d0609c-e849-41be-82cb-a599370bd799",
      "variant_type": "raw_html",
      "version": 1,
      "created_by": "evernote_ingest:v0",
      "created_at": "2025-11-10T15:43:32.250286Z",
      "content": "---\ntitle: 利用爬虫技术能做到哪些很酷很有趣很有用的事情？\nupdated: 2017-02-22 15:52:59Z\ncreated: 2017-02-22 15:52:59Z\ntags:\n  - 知乎收藏\n---\n\n\n\n<en-note><p>爬图片，你懂的=。=，比如这个网站：<a HREF=\"http://link.zhihu.com/?target=http%3A//mm36d.com\"><span>http://</span><span>mm36d.com</span><span></span><i></i></a></p><p>环境：</p><p>Python 2.7，request、BeautifulSoup三方库</p><p>见代码：</p><div><blockquote><code># -*- coding: utf8 -*-\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport os\n\n\ndef main():\n    result_root_path = &apos;mm36d所有图片/&apos;  # 图片保存目录\n    request_root_url = &apos;http://mm36d.com/belle/1/1/&apos;  # 根URL\n    request_param_1 = 0  # 参数1\n    total_image_count = 0  # 图片递增总数\n\n    # 创建文件夹\n    if not os.path.exists(result_root_path):\n        os.makedirs(result_root_path)\n\n    while True:\n        request_param_1 += 1  # 参数1加1\n        request_param_2 = 0  # 参数2\n\n        # 请求头\n        headers = {\n            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1) Gecko/20090624 Firefox/3.5 &apos;,\n            &apos;Accept&apos;: &apos;text/plain&apos;,\n            &apos;Connection&apos;: &apos;close&apos;\n        }\n\n        while True:\n            request_param_2 += 1  # 参数2加1\n            request_url = request_root_url + str(request_param_1) + &apos;/&apos; + str(request_param_2)  # 拼接URL\n\n            print(&apos;Page: &apos; + request_url)\n\n            requests.session().keep_alive = False\n\n            try:\n                # 获取页面内容\n                response = requests.get(request_url, headers=headers, allow_redirects=False)\n\n                # 请求成功\n                if response.status_code == 200:\n                    soup = BeautifulSoup(response.text, &apos;html.parser&apos;)  # BeautifulSoup解析\n                    response.close()  # 关闭请求\n                    image_url = soup.find(&apos;img&apos;, {&apos;class&apos;: &apos;lazy&apos;})[&apos;data-original&apos;]  # 提取图片URL\n\n                    response = requests.get(image_url)  # 下载图片\n                    image_file = open(result_root_path + str(total_image_count) + &apos;.jpg&apos;, &apos;wb&apos;)  # 创建图片文件\n                    image_file.write(response.content)  # 写图片\n                    image_file.close()  # 关闭文件\n                    response.close()  # 关闭请求\n\n                    total_image_count += 1  # 计数增加\n                    print(image_url + &apos; done. Current: &apos; + str(total_image_count))\n                else:\n                    print(&apos;Next page&apos;)\n                    break\n            except Exception as e:\n                print(&apos;Error: &apos; + e.__doc__)\n\n\nif __name__ == &apos;__main__&apos;:\n    main()\n</code></blockquote></div><p>更多详情: <a HREF=\"http://www.zhihu.com/question/27621722/answer/146022005\">http://www.zhihu.com/question/27621722/answer/146022005</a></p></en-note>      ",
      "content_path": null,
      "diff_base_variant_id": null,
      "metadata": {
        "checksum": "447d390c556476b2de5905e2617ac2e7d854a594bfc6e7f8253de9353322f615",
        "path": "backups/2023年6月/IT技术/利用爬虫技术能做到哪些很酷很有趣很有用的事情？.html"
      }
    },
    {
      "id": "f4e650a9-e6f9-4507-9938-a2a06b925dd5",
      "note_id": "a6d0609c-e849-41be-82cb-a599370bd799",
      "variant_type": "clean_text",
      "version": 1,
      "created_by": "evernote_ingest:v0",
      "created_at": "2025-11-10T15:43:32.250289Z",
      "content": "---\ntitle: 利用爬虫技术能做到哪些很酷很有趣很有用的事情？\nupdated: 2017-02-22 15:52:59Z\ncreated: 2017-02-22 15:52:59Z\ntags:\n  - 知乎收藏\n---\n\n爬图片，你懂的=。=，比如这个网站：\nhttp://\nmm36d.com\n环境：\nPython 2.7，request、BeautifulSoup三方库\n见代码：\n# -*- coding: utf8 -*-\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport os\n\ndef main():\n    result_root_path = 'mm36d所有图片/'  # 图片保存目录\n    request_root_url = 'http://mm36d.com/belle/1/1/'  # 根URL\n    request_param_1 = 0  # 参数1\n    total_image_count = 0  # 图片递增总数\n\n    # 创建文件夹\n    if not os.path.exists(result_root_path):\n        os.makedirs(result_root_path)\n\n    while True:\n        request_param_1 += 1  # 参数1加1\n        request_param_2 = 0  # 参数2\n\n        # 请求头\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1) Gecko/20090624 Firefox/3.5 ',\n            'Accept': 'text/plain',\n            'Connection': 'close'\n        }\n\n        while True:\n            request_param_2 += 1  # 参数2加1\n            request_url = request_root_url + str(request_param_1) + '/' + str(request_param_2)  # 拼接URL\n\n            print('Page: ' + request_url)\n\n            requests.session().keep_alive = False\n\n            try:\n                # 获取页面内容\n                response = requests.get(request_url, headers=headers, allow_redirects=False)\n\n                # 请求成功\n                if response.status_code == 200:\n                    soup = BeautifulSoup(response.text, 'html.parser')  # BeautifulSoup解析\n                    response.close()  # 关闭请求\n                    image_url = soup.find('img', {'class': 'lazy'})['data-original']  # 提取图片URL\n\n                    response = requests.get(image_url)  # 下载图片\n                    image_file = open(result_root_path + str(total_image_count) + '.jpg', 'wb')  # 创建图片文件\n                    image_file.write(response.content)  # 写图片\n                    image_file.close()  # 关闭文件\n                    response.close()  # 关闭请求\n\n                    total_image_count += 1  # 计数增加\n                    print(image_url + ' done. Current: ' + str(total_image_count))\n                else:\n                    print('Next page')\n                    break\n            except Exception as e:\n                print('Error: ' + e.__doc__)\n\nif __name__ == '__main__':\n    main()\n\n更多详情:\nhttp://www.zhihu.com/question/27621722/answer/146022005",
      "content_path": null,
      "diff_base_variant_id": null,
      "metadata": {
        "language": "en",
        "length": 2307,
        "rule_count": 1,
        "applied_rules": [
          {
            "rule_id": "whitespace",
            "description": "Normalize whitespace",
            "note": "collapsed whitespace"
          }
        ]
      }
    }
  ],
  "extractions": [
    {
      "id": "8f4aac5f-adeb-433c-b508-c192efd4b7c2",
      "note_id": "a6d0609c-e849-41be-82cb-a599370bd799",
      "extractor": "llm_enhance:v0#qwen3:30b",
      "payload": {
        "summary": "使用Python爬虫技术自动抓取图片网站的图片，示例代码涉及请求、解析和保存图片，适用于图片收集场景。",
        "keywords": [
          "爬虫",
          "Python",
          "图片抓取",
          "BeautifulSoup",
          "requests"
        ],
        "action_items": [
          "无"
        ],
        "source": "qwen3:30b",
        "category_path": [
          "多媒体",
          "图片"
        ],
        "new_category_suggestion": null
      },
      "version": 1,
      "created_at": "2025-11-12T03:32:15.884558Z",
      "created_by": "llm_enhance:v0",
      "quality_score": 0.318
    }
  ],
  "journal": {
    "id": "c35ba459-cffa-4202-8bf6-d2a203e771ad",
    "note_id": "a6d0609c-e849-41be-82cb-a599370bd799",
    "stage": "ingest",
    "agent_id": "evernote_ingest:v0",
    "started_at": "2025-11-10T15:43:32.250294Z",
    "finished_at": "2025-11-10T15:43:32.250294Z",
    "status": "success",
    "input_ref": {
      "task_id": "4b1769ef-13c7-4602-b1f6-dc813e90a8e5",
      "source_path": "backups/2023年6月/IT技术/利用爬虫技术能做到哪些很酷很有趣很有用的事情？.html",
      "checksum": "447d390c556476b2de5905e2617ac2e7d854a594bfc6e7f8253de9353322f615"
    },
    "output_ref": {
      "ingest_source": "dc1d0f8c-15f2-4141-9b9a-b0e97d77a453",
      "note": "a6d0609c-e849-41be-82cb-a599370bd799",
      "variants": [
        "366e2097-307f-4847-b0a6-a52a62e2375d",
        "f4e650a9-e6f9-4507-9938-a2a06b925dd5"
      ]
    },
    "error_detail": null
  },
  "llm": {
    "status": "success",
    "model": "qwen3:30b",
    "updated_at": "2025-11-12T03:32:15.884369Z",
    "latency_seconds": 26.096061374992132,
    "attempts": 1,
    "summary": {
      "summary": "使用Python爬虫技术自动抓取图片网站的图片，示例代码涉及请求、解析和保存图片，适用于图片收集场景。",
      "keywords": [
        "爬虫",
        "Python",
        "图片抓取",
        "BeautifulSoup",
        "requests"
      ],
      "action_items": [
        "无"
      ],
      "source": "qwen3:30b",
      "category_path": [
        "多媒体",
        "图片"
      ],
      "new_category_suggestion": null
    },
    "quality": {
      "score": 0.318,
      "metrics": {
        "input_chars": 2307.0,
        "input_lines": 63.0,
        "summary_chars": 51.0,
        "summary_coverage_ratio": 0.022,
        "keyword_hit_rate": 0.8,
        "action_item_count": 1.0,
        "unique_summary_sentences": 1.0,
        "estimated_read_seconds": 138.4
      }
    }
  }
}